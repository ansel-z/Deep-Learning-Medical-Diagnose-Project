{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4RpK9pawQzP"
   },
   "source": [
    "# Breast cancer classification with Keras and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcrOk6pURp50"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qe4GszObV708"
   },
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SeparableConv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import shutil\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc7yTkCnWEcx"
   },
   "source": [
    "### Our `Config` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mUKqe94JWGAi"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # initialize the path to the *original* input directory of images\n",
    "    ORIG_INPUT_DATASET = \"data\"\n",
    "\n",
    "    # initialize the base path to the *new* directory that will contain\n",
    "    # our images after computing the training and testing split\n",
    "    BASE_PATH = \"data\"\n",
    "\n",
    "    # derive the training, validation, and testing directories\n",
    "    TRAIN_PATH = os.path.sep.join([BASE_PATH, \"training\"])\n",
    "    VAL_PATH = os.path.sep.join([BASE_PATH, \"validation\"])\n",
    "    TEST_PATH = os.path.sep.join([BASE_PATH, \"testing\"])\n",
    "\n",
    "    # define the amount of data that will be used training\n",
    "    TRAIN_SPLIT = 0.8\n",
    "\n",
    "    # the amount of validation data will be a percentage of the\n",
    "    # *training* data\n",
    "    VAL_SPLIT = 0.1\n",
    "\n",
    "# initialize our config class\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YHP4XwPbjZT"
   },
   "source": [
    "### Building the breast cancer image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VVOuQlGLbm9b"
   },
   "outputs": [],
   "source": [
    "# grab the paths to all input images in the original input directory\n",
    "# and shuffle them\n",
    "imagePaths = list(paths.list_images(config.ORIG_INPUT_DATASET))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "# compute the training and testing split\n",
    "i = int(len(imagePaths) * config.TRAIN_SPLIT)\n",
    "trainPaths = imagePaths[:i]\n",
    "testPaths = imagePaths[i:]\n",
    "\n",
    "# we'll be using part of the training data for validation\n",
    "i = int(len(trainPaths) * config.VAL_SPLIT)\n",
    "valPaths = trainPaths[:i]\n",
    "trainPaths = trainPaths[i:]\n",
    "\n",
    "# define the datasets that we'll be building\n",
    "datasets = [\n",
    "    (\"training\", trainPaths, config.TRAIN_PATH),\n",
    "    (\"validation\", valPaths, config.VAL_PATH),\n",
    "    (\"testing\", testPaths, config.TEST_PATH)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m-Xf3956b1Z8"
   },
   "outputs": [],
   "source": [
    "# loop over the datasets\n",
    "\n",
    "# for (dType, imagePaths, baseOutput) in datasets:\n",
    "\n",
    "#     # show which data split we are creating\n",
    "#     print(\"[INFO] building '{}' split\".format(dType))\n",
    "    \n",
    "#     # if the output base output directory does not exist, create it\n",
    "#     if not os.path.exists(baseOutput):\n",
    "#         print(\"[INFO] 'creating {}' directory\".format(baseOutput))\n",
    "#         os.makedirs(baseOutput)\n",
    "\n",
    "#     # loop over the input image paths\n",
    "#     for inputPath in imagePaths:\n",
    "#         # extract the filename of the input image and extract the\n",
    "#         # class label (\"0\" for \"negative\" and \"1\" for \"positive\")\n",
    "#         filename = inputPath.split(os.path.sep)[-1]\n",
    "#         label = filename[-5:-4]\n",
    "        \n",
    "#         # build the path to the label directory\n",
    "#         labelPath = os.path.sep.join([baseOutput, label])\n",
    "\n",
    "#         # if the label output directory does not exist, create it\n",
    "#         if not os.path.exists(labelPath):\n",
    "#             print(\"[INFO] 'creating {}' directory\".format(labelPath))\n",
    "#             os.makedirs(labelPath)\n",
    "\n",
    "#         # construct the path to the destination image and then copy\n",
    "#         # the image itself\n",
    "#         p = os.path.sep.join([labelPath, filename])\n",
    "#         shutil.copy2(inputPath, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXSlbE6menJN"
   },
   "source": [
    "### CancerNet: Our breast cancer prediction CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WYskqW8wb-ll"
   },
   "outputs": [],
   "source": [
    "class CancerNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "\n",
    "        # CONV => RELU => POOL\n",
    "        model.add(SeparableConv2D(32, (3, 3), padding=\"same\",\n",
    "            input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # (CONV => RELU => POOL) * 2\n",
    "        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # (CONV => RELU => POOL) * 3\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # first (and only) set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4gB9XpXeuoT"
   },
   "source": [
    "### Our training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dtBvB8BWerjl"
   },
   "outputs": [],
   "source": [
    "# # construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
    "# \thelp=\"path to output loss/accuracy plot\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "# since we are using Jupyter Notebooks we can replace our argument\n",
    "# parsing code with *hard coded* arguments and values\n",
    "args = {\n",
    "    \"plot\": \"plot.png\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mkFT2du7fjP2"
   },
   "outputs": [],
   "source": [
    "# initialize our number of epochs, initial learning rate, and batch\n",
    "# size\n",
    "NUM_EPOCHS = 40\n",
    "INIT_LR = 1e-2\n",
    "BS = 32\n",
    "\n",
    "# determine the total number of image paths in training, validation,\n",
    "# and testing directories\n",
    "trainPaths = list(paths.list_images(config.TRAIN_PATH))\n",
    "totalTrain = len(trainPaths)\n",
    "totalVal = len(list(paths.list_images(config.VAL_PATH)))\n",
    "totalTest = len(list(paths.list_images(config.TEST_PATH)))\n",
    "\n",
    "# calculate the total number of training images in each class and\n",
    "# initialize a dictionary to store the class weights\n",
    "trainLabels = [int(p.split(os.path.sep)[-2]) for p in trainPaths]\n",
    "trainLabels = to_categorical(trainLabels)\n",
    "classTotals = trainLabels.sum(axis=0)\n",
    "classWeight = dict()\n",
    "\n",
    "# loop over all classes and calculate the class weight\n",
    "for i in range(0, len(classTotals)):\n",
    "    classWeight[i] = classTotals.max() / classTotals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "f3MrLFYFfnnd"
   },
   "outputs": [],
   "source": [
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(\n",
    "    rescale=1 / 255.0,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.05,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.05,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "# initialize the validation (and testing) data augmentation object\n",
    "valAug = ImageDataGenerator(rescale=1 / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sQqA8YmFf03_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 255556 images belonging to 2 classes.\n",
      "Found 42596 images belonging to 2 classes.\n",
      "Found 99743 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# initialize the training generator\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "    config.TRAIN_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    batch_size=BS)\n",
    "\n",
    "# initialize the validation generator\n",
    "valGen = valAug.flow_from_directory(\n",
    "    config.VAL_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=BS)\n",
    "\n",
    "# initialize the testing generator\n",
    "testGen = valAug.flow_from_directory(\n",
    "    config.TEST_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jZslqkNff7Q4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leobr\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\leobr\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/40\n",
      "7986/7986 [==============================] - 680s 85ms/step - loss: 0.6016 - accuracy: 0.8198 - val_loss: 0.4626 - val_accuracy: 0.8114\n",
      "Epoch 2/40\n",
      "7986/7986 [==============================] - 615s 77ms/step - loss: 0.5573 - accuracy: 0.8309 - val_loss: 0.4464 - val_accuracy: 0.8210\n",
      "Epoch 3/40\n",
      "7986/7986 [==============================] - 613s 77ms/step - loss: 0.5465 - accuracy: 0.8352 - val_loss: 0.4238 - val_accuracy: 0.8294\n",
      "Epoch 4/40\n",
      "7986/7986 [==============================] - 616s 77ms/step - loss: 0.5423 - accuracy: 0.8359 - val_loss: 0.4139 - val_accuracy: 0.8371\n",
      "Epoch 5/40\n",
      "7986/7986 [==============================] - 615s 77ms/step - loss: 0.5401 - accuracy: 0.8367 - val_loss: 0.4134 - val_accuracy: 0.8348\n",
      "Epoch 6/40\n",
      "7986/7986 [==============================] - 613s 77ms/step - loss: 0.5371 - accuracy: 0.8380 - val_loss: 0.4057 - val_accuracy: 0.8366\n",
      "Epoch 7/40\n",
      "7986/7986 [==============================] - 614s 77ms/step - loss: 0.5355 - accuracy: 0.8383 - val_loss: 0.4073 - val_accuracy: 0.8366\n",
      "Epoch 8/40\n",
      "7986/7986 [==============================] - 615s 77ms/step - loss: 0.5360 - accuracy: 0.8381 - val_loss: 0.4043 - val_accuracy: 0.8381\n",
      "Epoch 9/40\n",
      "7986/7986 [==============================] - 611s 76ms/step - loss: 0.5341 - accuracy: 0.8385 - val_loss: 0.4047 - val_accuracy: 0.8381\n",
      "Epoch 10/40\n",
      "7986/7986 [==============================] - 613s 77ms/step - loss: 0.5329 - accuracy: 0.8393 - val_loss: 0.4000 - val_accuracy: 0.8391\n",
      "Epoch 11/40\n",
      "7986/7986 [==============================] - 611s 77ms/step - loss: 0.5316 - accuracy: 0.8395 - val_loss: 0.3956 - val_accuracy: 0.8410\n",
      "Epoch 12/40\n",
      "7986/7986 [==============================] - 614s 77ms/step - loss: 0.5289 - accuracy: 0.8404 - val_loss: 0.4023 - val_accuracy: 0.8391\n",
      "Epoch 13/40\n",
      "7986/7986 [==============================] - 626s 78ms/step - loss: 0.5319 - accuracy: 0.8397 - val_loss: 0.4028 - val_accuracy: 0.8385\n",
      "Epoch 14/40\n",
      "7986/7986 [==============================] - 615s 77ms/step - loss: 0.5320 - accuracy: 0.8396 - val_loss: 0.3937 - val_accuracy: 0.8412\n",
      "Epoch 15/40\n",
      "7986/7986 [==============================] - 611s 77ms/step - loss: 0.5302 - accuracy: 0.8401 - val_loss: 0.3967 - val_accuracy: 0.8402\n",
      "Epoch 16/40\n",
      "7986/7986 [==============================] - 613s 77ms/step - loss: 0.5291 - accuracy: 0.8398 - val_loss: 0.3930 - val_accuracy: 0.8408\n",
      "Epoch 17/40\n",
      "7986/7986 [==============================] - 612s 77ms/step - loss: 0.5315 - accuracy: 0.8391 - val_loss: 0.3983 - val_accuracy: 0.8397\n",
      "Epoch 18/40\n",
      "7986/7986 [==============================] - 613s 77ms/step - loss: 0.5295 - accuracy: 0.8401 - val_loss: 0.3976 - val_accuracy: 0.8396\n",
      "Epoch 19/40\n",
      "7986/7986 [==============================] - 613s 77ms/step - loss: 0.5294 - accuracy: 0.8401 - val_loss: 0.3950 - val_accuracy: 0.8401\n",
      "Epoch 20/40\n",
      "7986/7986 [==============================] - 614s 77ms/step - loss: 0.5304 - accuracy: 0.8405 - val_loss: 0.3973 - val_accuracy: 0.8394\n",
      "Epoch 21/40\n",
      "7986/7986 [==============================] - 623s 78ms/step - loss: 0.5290 - accuracy: 0.8408 - val_loss: 0.3905 - val_accuracy: 0.8422\n",
      "Epoch 22/40\n",
      "7986/7986 [==============================] - 616s 77ms/step - loss: 0.5293 - accuracy: 0.8400 - val_loss: 0.3940 - val_accuracy: 0.8407\n",
      "Epoch 23/40\n",
      "7986/7986 [==============================] - 612s 77ms/step - loss: 0.5296 - accuracy: 0.8405 - val_loss: 0.3940 - val_accuracy: 0.8415\n",
      "Epoch 24/40\n",
      "7986/7986 [==============================] - 614s 77ms/step - loss: 0.5282 - accuracy: 0.8406 - val_loss: 0.3965 - val_accuracy: 0.8400\n",
      "Epoch 25/40\n",
      "7986/7986 [==============================] - 613s 77ms/step - loss: 0.5312 - accuracy: 0.8394 - val_loss: 0.3938 - val_accuracy: 0.8407\n",
      "Epoch 26/40\n",
      "7986/7986 [==============================] - 615s 77ms/step - loss: 0.5298 - accuracy: 0.8407 - val_loss: 0.3960 - val_accuracy: 0.8398\n",
      "Epoch 27/40\n",
      "7986/7986 [==============================] - 618s 77ms/step - loss: 0.5293 - accuracy: 0.8407 - val_loss: 0.3943 - val_accuracy: 0.8407\n",
      "Epoch 28/40\n",
      "7986/7986 [==============================] - 617s 77ms/step - loss: 0.5287 - accuracy: 0.8405 - val_loss: 0.3962 - val_accuracy: 0.8400\n",
      "Epoch 29/40\n",
      "7986/7986 [==============================] - 614s 77ms/step - loss: 0.5281 - accuracy: 0.8406 - val_loss: 0.3940 - val_accuracy: 0.8406\n",
      "Epoch 30/40\n",
      "7986/7986 [==============================] - 618s 77ms/step - loss: 0.5278 - accuracy: 0.8409 - val_loss: 0.3910 - val_accuracy: 0.8418\n",
      "Epoch 31/40\n",
      "7986/7986 [==============================] - 614s 77ms/step - loss: 0.5289 - accuracy: 0.8400 - val_loss: 0.3956 - val_accuracy: 0.8403\n",
      "Epoch 32/40\n",
      "7986/7986 [==============================] - 616s 77ms/step - loss: 0.5290 - accuracy: 0.8409 - val_loss: 0.3903 - val_accuracy: 0.8423\n",
      "Epoch 33/40\n",
      "7986/7986 [==============================] - 617s 77ms/step - loss: 0.5279 - accuracy: 0.8407 - val_loss: 0.3923 - val_accuracy: 0.8420\n",
      "Epoch 34/40\n",
      "7986/7986 [==============================] - 616s 77ms/step - loss: 0.5259 - accuracy: 0.8413 - val_loss: 0.3940 - val_accuracy: 0.8409\n",
      "Epoch 35/40\n",
      "7986/7986 [==============================] - 616s 77ms/step - loss: 0.5272 - accuracy: 0.8417 - val_loss: 0.3906 - val_accuracy: 0.8418\n",
      "Epoch 36/40\n",
      "7986/7986 [==============================] - 618s 77ms/step - loss: 0.5285 - accuracy: 0.8405 - val_loss: 0.3900 - val_accuracy: 0.8422\n",
      "Epoch 37/40\n",
      "7986/7986 [==============================] - 621s 78ms/step - loss: 0.5287 - accuracy: 0.8409 - val_loss: 0.3904 - val_accuracy: 0.8419\n",
      "Epoch 38/40\n",
      "7986/7986 [==============================] - 623s 78ms/step - loss: 0.5268 - accuracy: 0.8410 - val_loss: 0.3919 - val_accuracy: 0.8409\n",
      "Epoch 39/40\n",
      "7986/7986 [==============================] - 621s 78ms/step - loss: 0.5269 - accuracy: 0.8407 - val_loss: 0.3932 - val_accuracy: 0.8411\n",
      "Epoch 40/40\n",
      "7986/7986 [==============================] - 617s 77ms/step - loss: 0.5275 - accuracy: 0.8407 - val_loss: 0.3931 - val_accuracy: 0.8414\n",
      "INFO:tensorflow:Assets written to: Breast_cancer_model\\assets\n"
     ]
    }
   ],
   "source": [
    "### initialize our CancerNet model and compile it\n",
    "model = CancerNet.build(width=48, height=48, depth=3,\n",
    "    classes=2)\n",
    "opt = Adagrad(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# fit the model\n",
    "H = model.fit(\n",
    "    x=trainGen,\n",
    "    steps_per_epoch=totalTrain // BS,\n",
    "    validation_data=valGen,\n",
    "    validation_steps=totalVal // BS,\n",
    "    class_weight=classWeight,\n",
    "    epochs=NUM_EPOCHS)\n",
    "\n",
    "# save model\n",
    "model.save(\"Breast_cancer_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "I6GVGPXZhdSL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.89     71295\n",
      "           1       0.69      0.82      0.75     28448\n",
      "\n",
      "    accuracy                           0.85     99743\n",
      "   macro avg       0.81      0.84      0.82     99743\n",
      "weighted avg       0.86      0.85      0.85     99743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reset the testing generator and then use our trained model to\n",
    "# make predictions on the data\n",
    "print(\"[INFO] evaluating network...\")\n",
    "testGen.reset()\n",
    "predIdxs = model.predict(x=testGen, steps=(totalTest // BS) + 1)\n",
    "\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(sklearn.metrics.classification_report(testGen.classes, predIdxs,\n",
    "    target_names=testGen.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nTOBovQ4hkqk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60915 10380]\n",
      " [ 5019 23429]]\n",
      "acc: 0.8456\n",
      "sensitivity: 0.8544\n",
      "specificity: 0.8236\n"
     ]
    }
   ],
   "source": [
    "# compute the confusion matrix and and use it to derive the raw\n",
    "# accuracy, sensitivity, and specificity\n",
    "cm = sklearn.metrics.confusion_matrix(testGen.classes, predIdxs)\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
    "print(cm)\n",
    "print(\"acc: {:.4f}\".format(acc))\n",
    "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"specificity: {:.4f}\".format(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AuE0cvLghsic"
   },
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "N = NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(args[\"plot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sef8alx4cGYc"
   },
   "source": [
    "# Code License Agreement\n",
    "```\n",
    "Copyright (c) 2020 PyImageSearch.com\n",
    "\n",
    "SIMPLE VERSION\n",
    "Feel free to use this code for your own projects, whether they are\n",
    "purely educational, for fun, or for profit. THE EXCEPTION BEING if\n",
    "you are developing a course, book, or other educational product.\n",
    "Under *NO CIRCUMSTANCE* may you use this code for your own paid\n",
    "educational or self-promotional ventures without written consent\n",
    "from Adrian Rosebrock and PyImageSearch.com.\n",
    "\n",
    "LONGER, FORMAL VERSION\n",
    "Permission is hereby granted, free of charge, to any person obtaining\n",
    "a copy of this software and associated documentation files\n",
    "(the \"Software\"), to deal in the Software without restriction,\n",
    "including without limitation the rights to use, copy, modify, merge,\n",
    "publish, distribute, sublicense, and/or sell copies of the Software,\n",
    "and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be\n",
    "included in all copies or substantial portions of the Software.\n",
    "Notwithstanding the foregoing, you may not use, copy, modify, merge,\n",
    "publish, distribute, sublicense, create a derivative work, and/or\n",
    "sell copies of the Software in any work that is designed, intended,\n",
    "or marketed for pedagogical or instructional purposes related to\n",
    "programming, coding, application development, or information\n",
    "technology. Permission for such use, copying, modification, and\n",
    "merger, publication, distribution, sub-licensing, creation of\n",
    "derivative works, or sale is expressly withheld.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n",
    "OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
    "NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n",
    "BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n",
    "ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "“breast_cancer_classification.ipynb”的副本",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
